# Functional Tests
#WIDTH: 10,14,26,33,26,38,33,10,12
#ALIGN: left,left,left,left,left,left,left,center,center
Test ID,Test Type,Test Name,Description,Pre-conditions,Test Steps,Expected Results,Priority,Status
FT-001,Unit,DGX Node Boot Validation,Verify each DGX H100 node boots successfully and reports correct GPU count,DGX OS installed; drivers configured,1. Power on DGX node; 2. Monitor boot sequence; 3. Run nvidia-smi; 4. Verify 8 GPUs detected,Node boots successfully with 8x H100 GPUs visible in nvidia-smi,Critical,Not Started
FT-002,Unit,GPU Memory Validation,Verify each H100 GPU reports 80GB HBM3 memory,DGX node operational,1. Run nvidia-smi -q; 2. Check memory for each GPU; 3. Verify all 8 GPUs report 80GB,All 8 GPUs report 80GB HBM3 memory (81920 MiB),Critical,Not Started
FT-003,Unit,NVLink Connectivity Test,Verify NVLink fabric between GPUs within each DGX node,DGX node operational,1. Run nvidia-smi topo -m; 2. Verify NVLink connections; 3. Check bandwidth,NVLink topology shows all GPU pairs connected at 900 GB/s,High,Not Started
FT-004,Integration,InfiniBand Port Connectivity,Verify all InfiniBand ports are connected and operational,IB fabric cabled; switches configured,1. Run ibstat on each DGX; 2. Check port state; 3. Verify LinkUp status,All 8 IB ports per DGX show Active/LinkUp state at 400 Gbps,Critical,Not Started
FT-005,Integration,RDMA Communication Test,Verify RDMA communication between DGX nodes via InfiniBand,IB fabric operational,1. Run ib_send_bw between node pairs; 2. Run ib_send_lat; 3. Record throughput and latency,Bandwidth >380 Gbps per port; latency <2 microseconds,Critical,Not Started
FT-006,Integration,GPUDirect RDMA Validation,Verify GPUDirect RDMA enabled for direct GPU-to-GPU transfers,IB fabric operational; GDR module loaded,1. Run NCCL tests with GDR; 2. Verify GPU memory used directly; 3. Check for host memory bypass,NCCL uses GPU memory directly; no host staging observed,High,Not Started
FT-007,Integration,Storage Mount Validation,Verify Base Command storage accessible from all DGX nodes,Storage cluster deployed; NFS/parallel FS configured,1. Mount shared storage on each node; 2. Create test files; 3. Verify cross-node access,Storage mounted at /shared on all nodes; files accessible,Critical,Not Started
FT-008,System,Slurm Job Submission,Verify job submission and execution via Slurm scheduler,Slurm configured; user accounts created,1. Submit single-GPU job; 2. Submit multi-GPU job; 3. Verify resource allocation; 4. Check job completion,Jobs execute with requested GPU allocation; exit code 0,Critical,Not Started
FT-009,System,Multi-Node Job Execution,Verify multi-node distributed training job execution,Slurm operational; IB fabric validated,1. Submit 2-node job (16 GPUs); 2. Submit 8-node job (64 GPUs); 3. Verify all nodes participate,Multi-node jobs execute with all requested nodes; NCCL initializes,Critical,Not Started
FT-010,System,NGC Container Execution,Verify NGC container pull and execution on DGX nodes,NGC registry configured; container runtime installed,1. Pull PyTorch NGC container; 2. Run container with GPU access; 3. Execute GPU workload,Container runs with GPU access; CUDA operations succeed,High,Not Started
FT-011,System,User Authentication (LDAP),Verify LDAP/AD user authentication and SSH access,LDAP integrated; user accounts synced,1. SSH to DGX with LDAP user; 2. Verify home directory creation; 3. Check group membership,LDAP users authenticate successfully; correct permissions applied,High,Not Started
FT-012,System,Monitoring Dashboard Access,Verify DCGM metrics visible in Grafana dashboards,DCGM exporter running; Grafana configured,1. Access Grafana dashboard; 2. Verify GPU metrics displayed; 3. Check historical data,GPU utilization; temperature; memory metrics displayed correctly,Medium,Not Started

# Non-Functional Tests
#WIDTH: 10,14,22,38,30,44,38,10,12
#ALIGN: left,left,left,left,left,left,left,center,center
Test ID,Test Type,Test Name,Description,Pre-conditions,Test Steps,Expected Results,Priority,Status
NFT-001,Performance,Single-Node GPU Benchmark,Validate single DGX H100 achieves 4 petaFLOPS FP8 performance,DGX node operational; benchmarks installed,1. Run MLPerf BERT benchmark; 2. Run ResNet-50 benchmark; 3. Record throughput metrics,Single node achieves >4 petaFLOPS FP8 equivalent throughput,Critical,Not Started
NFT-002,Performance,Multi-Node Scaling Test,Validate near-linear scaling from 1 to 8 DGX nodes (64 GPUs),All nodes operational; IB fabric validated,1. Run NCCL all-reduce at 8/16/32/64 GPUs; 2. Calculate scaling efficiency; 3. Compare to linear,Scaling efficiency >90% at 64 GPUs; aggregate >30 petaFLOPS,Critical,Not Started
NFT-003,Performance,InfiniBand Fabric Bandwidth,Validate InfiniBand fabric achieves 3.2 Tbps aggregate bandwidth,All IB switches and cables installed,1. Run all-to-all bandwidth test; 2. Measure bisection bandwidth; 3. Check for bottlenecks,Aggregate bisection bandwidth >3.0 Tbps,Critical,Not Started
NFT-004,Performance,InfiniBand Latency Validation,Validate GPU-to-GPU latency <2 microseconds via InfiniBand,IB fabric operational; GDR enabled,1. Run ib_send_lat between all node pairs; 2. Measure p50 and p99 latency; 3. Record results,Average latency <2μs; p99 latency <3μs,Critical,Not Started
NFT-005,Performance,Storage Throughput Test,Validate Base Command storage achieves 14 GB/s sustained throughput,Storage cluster operational; mounted on all nodes,1. Run IOR benchmark from all nodes; 2. Test read and write; 3. Measure aggregate throughput,Aggregate read >14 GB/s; write >12 GB/s sustained,Critical,Not Started
NFT-006,Performance,NCCL All-Reduce Performance,Validate NCCL all-reduce bandwidth matches InfiniBand line rate,NCCL installed; GDR enabled,1. Run nccl-tests all_reduce_perf; 2. Test message sizes 1MB to 1GB; 3. Record bus bandwidth,Bus bandwidth >350 GB/s for large messages (64 GPUs),High,Not Started
NFT-007,Performance,Training Throughput Test,Validate real AI workload achieves expected training throughput,Full stack operational; NGC container available,1. Run GPT-3 training benchmark; 2. Measure tokens per second; 3. Compare to baseline,Training throughput >10x cloud baseline (AWS p4d.24xlarge),High,Not Started
NFT-008,Reliability,System Uptime Validation,Validate DGX SuperPOD achieves 99.5% uptime target,System operational for 7+ days,1. Monitor uptime over test period; 2. Record any outages; 3. Calculate availability,System availability >99.5% over test period,High,Not Started
NFT-009,Reliability,Node Failover Test,Validate Slurm handles node failure gracefully,Multi-node job running,1. Simulate node failure during job; 2. Verify job termination; 3. Check node marked down in Slurm,Slurm detects failure; job terminates cleanly; resources reallocated,High,Not Started
NFT-010,Reliability,InfiniBand Redundancy Test,Validate IB fabric handles link failure with redundancy,Redundant IB connections per node,1. Disable one IB port; 2. Run bandwidth test; 3. Verify continued operation,System operates with reduced bandwidth; no job failures,Medium,Not Started
NFT-011,Security,Access Control Validation,Validate role-based access control for DGX resources,LDAP integrated; Slurm ACLs configured,1. Test user access to assigned partitions; 2. Test unauthorized partition access; 3. Verify rejection,Users access only authorized resources; unauthorized access denied,High,Not Started
NFT-012,Security,SSH Key Authentication,Validate SSH key-based authentication enforced,SSH configured; password auth disabled,1. Attempt SSH with password; 2. Attempt SSH with valid key; 3. Attempt SSH with invalid key,Only SSH key authentication succeeds; password attempts rejected,High,Not Started
NFT-013,Security,Network Segmentation Test,Validate network isolation between management and compute networks,Network VLANs configured,1. Attempt cross-VLAN access; 2. Verify management traffic isolated; 3. Check firewall rules,Management and compute networks properly isolated,Medium,Not Started
NFT-014,Security,Encryption Validation,Validate data encryption for storage and network traffic,Encryption configured,1. Verify storage encryption enabled; 2. Check TLS for management traffic; 3. Validate IB security,Data encrypted at rest and in transit; certificates valid,Medium,Not Started

# User Acceptance Tests
#WIDTH: 10,20,29,33,26,38,33,10,12
#ALIGN: left,left,left,left,left,left,left,center,center
Test ID,Test Type,Test Name,Description,Pre-conditions,Test Steps,Expected Results,Priority,Status
UAT-001,Business Process,Data Scientist Workflow,Validate end-to-end data scientist workflow from login to job completion,User account created; training environment ready,1. SSH to login node; 2. Submit training job via Slurm; 3. Monitor with squeue; 4. Retrieve results,User completes full workflow; job results accessible,Critical,Not Started
UAT-002,Business Process,Distributed Training Job,Validate multi-node distributed training with production workload,All nodes operational; PyTorch/TensorFlow available,1. Prepare distributed training script; 2. Submit multi-node job; 3. Verify scaling; 4. Check convergence,Training scales across nodes; model converges correctly,Critical,Not Started
UAT-003,Business Process,Model Checkpointing,Validate model checkpoint save and restore workflow,Shared storage accessible; training job running,1. Configure checkpoint saving; 2. Run training with checkpoints; 3. Restart from checkpoint; 4. Verify continuation,Checkpoints saved successfully; training resumes from checkpoint,High,Not Started
UAT-004,Business Process,JupyterHub Interactive Session,Validate JupyterHub access for interactive development,JupyterHub deployed; GPU allocation configured,1. Login to JupyterHub; 2. Request GPU resources; 3. Run notebook with GPU; 4. Verify CUDA operations,Interactive session with GPU access; CUDA operations succeed,High,Not Started
UAT-005,Business Process,NGC Container Customization,Validate custom container build and deployment workflow,NGC base container available; Docker/Singularity configured,1. Pull NGC base container; 2. Add custom packages; 3. Build custom container; 4. Run on DGX,Custom container builds and runs with GPU access,Medium,Not Started
UAT-006,User Interface,Grafana Dashboard Usability,Validate monitoring dashboards meet operational requirements,Grafana deployed; DCGM metrics flowing,1. Review GPU utilization dashboard; 2. Check job queue dashboard; 3. Verify alert visibility,Dashboards provide actionable insights; metrics clear and accurate,Medium,Not Started
UAT-007,User Interface,Slurm Queue Visibility,Validate users can monitor job queue and resource availability,Slurm operational; multiple jobs submitted,1. Run squeue to view queue; 2. Run sinfo for node status; 3. Check estimated start time,Users can view queue position; resource availability clear,Medium,Not Started
UAT-008,Business Process,Large Model Training (175B),Validate training of GPT-3 scale model (175B parameters),All 64 GPUs available; training scripts ready,1. Configure 175B model training; 2. Submit 64-GPU job; 3. Monitor training metrics; 4. Validate throughput,175B model trains successfully; achieves expected throughput,Critical,Not Started
UAT-009,Business Process,Research Team Onboarding,Validate new researcher onboarding process,LDAP account created; documentation available,1. Follow onboarding documentation; 2. Complete first job submission; 3. Access shared datasets; 4. Run example training,New researcher productive within 2 hours of onboarding,High,Not Started
UAT-010,Business Process,Slurm Fair-Share Scheduling,Validate fair-share scheduling across research teams,Multiple teams configured; jobs from each team,1. Submit jobs from multiple teams; 2. Monitor resource allocation; 3. Verify fair-share ratios,Resources allocated according to team fair-share weights,Medium,Not Started
